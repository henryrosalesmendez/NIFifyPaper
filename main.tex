%
% This is based on the LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
% for Lecture Notes in Computer Science,
% version 2.4 for LaTeX2e as of 16. April 2010
%
% See http://www.springer.com/computer/lncs/lncs+authors?SGWID=0-40209-0-0-0
% for the full guidelines.
%
%

\documentclass{llncs}

\hyphenation{do-cu-ment}

\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}

\usepackage{graphicx}
\usepackage{comment}
\usepackage{xspace}
\usepackage[usenames, dvipsnames]{xcolor}
\definecolor{igual}{rgb}{0.21, 0.11, 0} 
\usepackage{hyperref}
\definecolor{dark-blue}{rgb}{0.0,0.0,0.1}
\definecolor{dark-green}{rgb}{0.0,0.1,0.0}
\definecolor{dark-red}{rgb}{0.1,0.0,0.0}
\hypersetup{
    colorlinks, linkcolor={dark-red},
    citecolor={dark-green}, urlcolor={dark-blue},
    pdftitle={NIFify: supportig NIF for EL},    % title
    pdfauthor={Henry Rosales-Méndez, Aidan Hogan, Barbara Poblete},     % author
    pdfsubject={ISWC 2018},   % subject of the document
    pdfkeywords={multilingual;} {entity linking;} {information extraction;} {benchmark;}, % list of keywords
}
\usepackage{amsmath}
\newcommand{\argmin}{\arg\!\min}
\newcommand{\argmax}{\arg\!\max}

%para el simbolo de chequeado
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{booktabs} 
\usepackage{multirow}

\newcommand{\ah}[1]{{\color{blue}\textsc{ah:} #1}}

\usepackage{soul} %middleline
\usepackage{pgfplots}

\begin{document}

\title{NIFify: supporting NIF for Entity Linking}
\titlerunning{NIFify: supporting NIF for EL}
\author{Henry Rosales-M\'endez, Aidan Hogan and Barbara Poblete}
\authorrunning{Rosales-Méndez et al.}
\institute{IMFD Chile \& Department of Computer Science, University of Chile \\
\texttt{\{hrosales,ahogan,bpoblete\}@dcc.uchile.cl}}

\newcommand{\ds}{\textsc{VoxEL}\xspace}

\maketitle  % typeset the title of the contribution
\begin{abstract}
The Entity Linking (EL) task identifies entity mentions in a text corpus and associates them with a corresponding unambiguous entry in a Knowledge Base. So far, the evaluation process of EL systems relies on the comparison of their results against gold standards. Despite that some gold standard has been proposed, they are still insufficient in some specific domains or situations that are commonly presented in real environments. % (e.g., multilingualism, social media, non-English language).
The most recents of them commonly have been created using the NIF format, which is focused in spotting the mentions in a text corpus and annotate them with URIs using RDF triples. In this paper we first propose a minor extension of NIF format in order to cover the entity type, and also we propose the tool NIFify for the creation, visualization and validation of NIF-based datasets, as well as for our poposed extension of NIF. NIFify also serves as benchmark tool that allows the assesment of EL systems. Additionally, we explore the quality of some popular NIF-based gold standards trough NIFify. 
\keywords{Entity Linking, Validation, NIF, Benchmark creation tool}\\
\end{abstract}



%---------------------------------------------
\section{Introduction} 
\label{sec:intro}

The Entity Linking (EL) task identifies entity mentions in a text corpus and associates them with corresponding unambiguous entry in a Knowledge Base (KB). EL task belongs to the Information Extraction (IE) area, and it have gained the attention of the scientific community in the recent years, mainly because the increasing availability of huge KBs that cover a long portion of the whole knowledge (e.g., Wikipedia, DBpedia, Wikidata, BabelNet).

EL is directly applicable in Natural Language Processing (NLP), since associating entities with KB build a bridge from unstructured information to structured data, which provide relevant information and relationships about them. For instance, in the sentence \textit{``Jackson won an award as best-selling artist of the 1980s"} a EL systems should identify \textit{Jackson} as \texttt{dbr:Michael\_Jackson}\footnote{Throughout, we use prefixes according to http://prefix.cc}, and in this way, we know that he was the famous musician knows as The King of the Pop. Many related fields can leverage EL benefit such as Semantic search, Semantic annotation, Text enrichment, Entity summarization and Relation Extraction. 

Several EL approaches have been proposed so far, addressing each time more specific situations presented in real environments such as multilingualism, specific domains, noisy texts, short texts, semi-structure inputs, etc. KEA~\cite{KEA2016}, for instance, is proposed to accurately perform over tweets, that represent a source of short and noisy information. With this variety of requirements, it is necessary to have tagged data that allows ad-hoc evaluations for each of these contexts, which are know as \textit{gold standards} or \textit{benchmark datasets}. In this lines, the evaluation of EL systems take as input the text information from these datasets and compare their results against the dataset annotations. 

Commonly, benchmark datasets are built by expert humans achieving in that way a ground truth. So far, these datasets have been written in more than one format that difficult the experimentation process. Hellmann et al., proposed in \cite{NIFpaper} the NLP Interchange Format (NIF) in order to simplify the interoperability of NLP tools that handle annotations in text corpus. The format NIF is based on RDF/OWL triples, fitting predicates to standardize the access of the information of the annotations. Although its well adoption in recent research, this format does not allow the specification of entity type for all scenarios. Our first proposal in this paper is the enhancing of NIF, covering in that way some scenarios where current mechanisms for the specification of entity types are not suitable.

Despite the benefits of NIF, the creation of triples is a complex and time-consuming work, usually performed through tools. In~\cite{N3} the authors propose three NIF-based monolingual datasets, from text written in English and German that were tagged manually using their own tool that is not pubished so far. Looking for mistakes in datasets,  Kunal et al.,~\cite{Kunal2017} propose a guideline of steps to validate them, providing the system EAGLET that check a variety of quality rules. In that way, EAGLET tell us if datasets are reliable and ready to be evolved in the EL assesment. On the other hand, some authors have dedicated a lot of effort on standardize the assessment process providing benchmarking suits (e.g., GERBIL~\cite{gerbil-2015}, Orbis~\cite{Orbis2018}) that dispose state-of-the-art systems and datasets to settings EL comparison. Benchmark tools constitute a fundamental factor to achieve the reproducibility of experiments, which is crucial in the achievements of its comparability. However, all of these NIF operations were attempted using separated systems. In this paper we propose NIFify, one tool that allows at the same time the creation, visualization and validation of NIF-based datasets, as well as the comparison of EL systems. 

NIFify contains a variety of desirable properties, where some of them were incorporated during the construction of VoxEL~\cite{VoxEL2018}, a dataset that contains the same annotation aligned by each sentence/document for five languages. Additionally of the NIF specifications, NIFify is built also to handle our NIF extensions. 

%In follow ...

%Dataset de tweets: Analysis of named entity recognition and linking for tweets
%version 2 de NIF  http://persistence.uni-leipzig.org/nlp2rdf/specification/version.html
%http://dashboard.nlp2rdf.aksw.org/
%http://persistence.uni-leipzig.org/nlp2rdf/specification/api.html
%https://stp.lingfil.uu.se/~nivre/research/MaltXML.html


%-------------------------------------------------------------------------------
\section{Preliminars}
\label{sec:nif}


The usual way to evaluate EL systems is performed through benchmark datasets, which contains a text corpora and its corresponding annotations respect to KBs. Once we perform the EL over the text of the datasets, the system output is compared against the gold standard annotations in order to measure the quality of the system. Although the achievements in EL benchmarkings, there is not universal format to write benchmark dataset in EL, but more than one. One of the first proposed format to support EL annotation is proposed with the MSNBC~\cite{cucerzan2007large} dataset, which have two separated files, a plain text file and other file that contains the specifications of the annotations. This same format is followed by other authors in the proposition of EL datasets (e.g., ACE2004~\cite{aquaint}, AQUAINT~\cite{aquaint}, IITB~\cite{IITB2009}). 

Some formats employed to write EL datasets are based on XML (e.g., MSNBC, IITB~\cite{IITB2009}, RENDEN~\cite{renden2016}, CAT~\cite{meantime2016}) or CSV (e.g., AIDA~\cite{aida2011}, SemEval~\cite{moro2015semeval}). For a general goal, Melo et al.~\cite{Lexvo2008} propose Lexvo\footnote{\url{http://lexvo.org/ontology}; January 1st, 2019.}, a RDF-based format and service that defines a unique URI for terms, languages, scripts, and characters from a text corpus, allowing their use in Semantic Web. %In this context, Melo et al.~\cite{Lexvo2008} propose the Lexvo.org service and Lexvo Ontology\footnote{\url{http://lexvo.org/ontology}; January 1st, 2019.}, that allow the constructions of human-readable and machine-readable data based on RDF triples. Lexvo defines a unique URI for terms, languages, scripts, and characters for use in Semantic Web; and provide links to several thesauri and KBs such as Wiktionary and Wikipedia. 
On the other hand, Hellmann et al., proposed in \cite{NIFpaper} the NLP Interchange Format (NIF) which supports the interoperability of a variaty of NLP tools, used by several of the last EL datasets (e.g., N3-RSS 500~\cite{N3}, Reuters 128~\cite{N3}, Wes2015~\cite{wes2015}, News-100~\cite{N3}, DBpedia Abstracts~\cite{abstracts2016}, VoxEL~\cite{VoxEL2018}). With the idea of homogenize the use of NIF-based datasets, some authors have been transformed popular non-NIF datasets to this format, this is the case of  KORE50 and DBpedia Spotlight\footnote{\url{http://apps.yovisto.com/labs/ner-benchmarks}; January 1st, 2019.}. In Table~\ref{tab:datasets} we list the most popular EL datasets in the literature and some details of their constructions.

\newcommand{\ccell}[1]{\multicolumn{1}{c}{#1}}
\newcommand{\rcell}[1]{\multicolumn{1}{r}{#1}}
\setlength{\tabcolsep}{1.2ex}
\begin{table}[tb!]
%\begin{table}[!]
\centering
\caption{Survey of dataset for EL task. We highlighted in bold those datasets that have been re-writing to NIF format.}
\label{tab:datasets} 
\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrccc}
\toprule
\textbf{Dataset}~~~~~~~~~~~~~~~~~~ & \ccell{Year}&\rcell{$|D|$} & \rcell{$|S|$} & \rcell{$|E|$} & \ccell{\textbf{Mn}} & \ccell{\textbf{Typ}}&\ccell{\textbf{Format}}\\\midrule
MSNBC~\cite{cucerzan2007large}      &2007&20      &668    &747     &\xmark &\xmark & MSNBC$_{xml}$\\\midrule
IITB~\cite{IITB2009}                &2009&103     &1,781  &18,308  &\cmark &\xmark & IITB$_xml$\\\midrule
AIDA/CoNLL-Complete~\cite{aida2011} &2011&1393    &22,137 &34,929  &\cmark &\xmark & AIDA$_{csv}$ \\\midrule
ACE2004~\cite{aquaint}              &2011&57      &-      &306     &\xmark &\xmark & MSNBC$_{xml}$\\\midrule
AQUAINT~\cite{aquaint}              &2011&50      &533    &727     &\xmark &\xmark & MSNBC$_{xml}$\\\midrule
\textbf{DBpedia Spotlight}
\cite{mendes2011dbpedia}            &2011&10      &58     &331     &\cmark &\xmark & Lexvo\\\midrule
\textbf{KORE50}~\cite{kore50}       &2012&50      &50     &144     &\cmark &\xmark & AIDA$_{csv}$\\\midrule
N3-RSS 500~\cite{N3}                &2014&1       &500    &1000    &\cmark &\xmark & NIF \\\midrule
Reuters 128~\cite{N3}               &2014&128     &-      &881     &\cmark &\xmark & NIF \\\midrule
News-100~\cite{N3}                  &2014&100     &-      &1656    &\cmark &\xmark & NIF \\\midrule
Wes2015~\cite{wes2015}              &2015&331     &-      &28,586  &\cmark &\xmark & NIF \\\midrule
SemEval 2015 
Task 13~\cite{moro2015semeval}      &2015&4       &137    &769     &\cmark &\xmark & SemEval$_{csv}$\\ \midrule
Thibaudet~\cite{renden2016}         &2016&1       &3,807  &2,980   &\xmark &\cmark & RENDEN$_{xml}$\\\midrule
Bergson~\cite{renden2016}           &2016&1       &4,280  &380     &\xmark &\cmark & RENDEN$_{xml}$\\\midrule
DBpedia Abstracts
~\cite{abstracts2016}               &2016&39,132  &-      &505,033 &\xmark &\xmark & NIF\\\midrule
MEANTIME~\cite{meantime2016}        &2016&120     &597    &2,790   &\cmark &\cmark & CAT$_{xml}$\\\midrule 
VoxEL$_R$~\cite{VoxEL2018}          &2018&15      &94     &674     &\cmark &\xmark & NIF\\\midrule  
VoxEL$_S$~\cite{VoxEL2018}          &2018&15      &94     &204     &\cmark &\xmark & NIF\\   
\bottomrule
\end{tabular}
}
\end{table}

The NIF format is based on RDF/OWL triples $<$\textit{subject}, \textit{predicate}, \textit{object}$>$ where the \textit{subjects} constitute units of information such as document, setences, and annotations; and the pair \textit{predicates-objects} define their properties. Predicates as \texttt{nif:beginIndex} and \texttt{nif:endIndex} indicate the start and end position of the entity mention in the sentence. The targetted KB resources is specified by NIF using the predicate \texttt{itsrdf:taIdentRef}, and the most specific class references can be defined by \texttt{itsrdf:taClassRef}. This group of predicates allows the entity mention and type specifications, while other predicates capture metadata related to other close tasks, such as Stemming (\texttt{nif:stem}) and Part-Of-Speech (\texttt{nif:oliaCategory}, \texttt{nif:lemma}).  




%Sentence: 
%Thomas and Mario are strikers playing in Munich.
%------------------------------------------------
%https://en.wikipedia.org/wiki/FC_Bayern_Munich
%https://en.wikipedia.org/wiki/Munich

%The US and the EU do not agree however on considering wether to supply military aid to Kiev.



% Towards Universal Multilingual Knowledge Bases
% Lexvo.org: Language-Related Information for the Linguistic Linked Data Cloud
% http://www.lexvo.org/linkeddata/resources.html%
% KORE50 y DBpedia Spotlight fueron transformadas en NIF (http://apps.yovisto.com/labs/ner-benchmarks)

 

%----------------------------------------------------------------------------------
\section{Proposition of NIF extension}
\label{sec:nifmod}

Due to its interoperability property, NIF have been enhanced from its current version 2.0 aiming the adoption of not supported specifications. One example of that is Wes2015, a NIF-based datasets for Document Retrieval that contains information about queries specified with ad-hoc predicates and classes (e.g., \texttt{si:Query}, \texttt{si:result}, \texttt{yv:queryId}) no included in the core NIF. In this Section we details the need of new mechanisms to specify the entity type according to the links and not related to the mentions, handling in this way, annotations that incorporate more than one link.


Entity type specifications is an valuable metadata in NLP, used commonly as indicator in the decision making of process that involve entities. The detection of entities type have been well studied so far, separated by some author as the subtask Entity Type Recognition (ETR) from Entity Recognition. ETR also have been stressed on international competitions as CALCS~\cite{calcs2018shtask}, including Tracks that aims the entity types prediction of the entities from a given text corpora. 

The entity type is specificated in NIF by the most-specific-class predicate (\texttt{itsrdf:taClassRef}) as shows one of the oficial example\footnote{\url{http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core/example.ttl}; January 1st, 2019.} of NIF group. However, with this solution some problematic situation emerge when the same annotation refers to more than one URI in the KB. This is due to the fact that either the context is not enough to fully disambiguate the entity mention but partially, or the entity mention is intrinsically ambiguous in its context. We elaborate two examples to shows this facts based on the following two sentences:

\begin{description}
\item[S1] \textit{``Bush was president of the United State of America"}
\item[S2] \textit{``Iran is not capable of developing a nuclear program without Moscow's help"}
\end{description}

The first example is based on the sentence S1, where there is not enough information to decide if the entity mention \textit{Bush} is refering to the 41rd US president George H. W. Bush or to his song, because both reach this position in the US government and they had also the same family name. Even so, we do have the information to know that this sentence is talking about one of them. On the other hand, the second example is based on the sentence S2, where the entity mention \textit{Moscow} in this context is intrinsically ambiguous because can refers either the Government of Russia (\texttt{wiki:Government\_of\_Russia}) or its capital (\texttt{wiki:Moscow}). 

%These multiple annotations of NIF introduce 

%Esta annotacipon multiple trae consigo un conflicto en los tipos de entidad, ya que cada
We expect that each link of the same annotation refers to the same entity type -- or in general to the same set of classes --, so that we can specify this property trough the predicate \texttt{itsrdf:taClassRef}. This is the case of the annotation of \textit{Bush} in sentence S1, because the resources associate to both president describe persons. However, it is not that what always happens in real environments. For example, in sentence S2 the mention \textit{Moscow} is targetting two URIs which different entity type, one is a \texttt{dbo:Place} and the other an \texttt{dbo:Organization} according to the DBpedia. 

Multiple links in the same annotation are already supported by NIF, but there are not distintion about the entity type of each links. Therefore, we propose to separate the entity type specification from the annotation scope defining a new triple $<s_i$, \texttt{\textcolor{red}{enif}:entityType} , $o_i>$ for each link in the annotation, whereas $s_n$ corresponds to the URI and $o_i$ the type of the entity. In Figure~\ref{fig:nif} we show the annotation of Moscow from sentence S2, followed by two triples corresponding to our extension. 

\begin{figure}
\caption{NIF triples to specify the annotation of Moscow from sentence S2. We include the last two triples that shows our proposal of NIF extension to specify entity types.}
\label{fig:nif}
\begin{Verbatim}[frame=single]
<http://example.org#char=88,94>
    a nif:String, nif:Context, nif:Phrase, nif:RFC5147String;
    nif:anchorOf """Moscow"""^^xsd:string;
    nif:beginIndex "88"^^xsd:nonNegativeInteger;
    nif:endIndex "94"^^xsd:nonNegativeInteger;
    itsrdf:taIdentRef <.../wiki/Moscow>;
    itsrdf:taIdentRef <.../wiki/Government_of_Russia>.  
    
<.../wiki/Moscow> enif:entityType dbo:Place.
<.../wiki/Government_of_Russia> enif:entityType dbo:Organization.
\end{Verbatim}
\end{figure}


%----------------------------------------------------------------------------------
\section{NIF Construction}
A plethora of systems have been proposed to operate with NIF, from the basic process of annotation, to other more complex as the validations and quality assessment of EL systems. The benchmark dataset creation is commonly performed manually, as you can see in Table \ref{tab:datasets}, and choosing the automatic annotation commonly for those scenarios where it is hard to perform a human intervension. This is the case of DBpedia Abstracts, a benchmark datasets based on the abstracts of Wikipedia which represent a source of information too large for human processing. \textcolor{red}{Neverless}, for both ways of construction it is not specified in all of these papers how they perform the annotations. In the case of the N$^3$ datasets~\cite{N3} (i.e., N3-RSS 500, Reuters 128, News-100), the authors developed an annotation tool to create them, but there is not information in the literature about where found it. 

One step to avoid the manually annotation process behind gold standard is provided by Ngoma et al.~\cite{Bengal2018}, who propose BENGAL, an automatic benchmark generator for \textcolor{red}{ER}/EL. BENGAL goes for the facts of current RDF information to text, which guaranteed to contain no errors. Despite the growing availability of RDF data, some context are difficult find in RDF format; in particular those that aim to capture specific situations (e.g., noisy text). %For instance, VoxEL could be difficult to obtain with an automatic process due to the source text from the different languages were written by journalists, who changes/delete/extends the original news in the translation from the original one. So, aligning the entity mentions of VoxEL without human intervention could be a \textcolor{blue}{complex problem.}
In this paper, we propose NIFify, a tool that allows the annotation and validation of NIF datasets in a same environment, as well as the comparison of EL systems. We can see in Table~\ref{tab:datasets} that current datasets contain more than one document, property captured by NIFify, which handle more than one text corpora at the same time. We design NIFify in order to capture a general definition of entity, allowing partial o total overlapping among them, as well as the cross-links specification. As a consequence of the annotation process, this tool is a suit to visualize and modify already proposed datasets in NIF. We include in NIFify a way to transform MSNBC$_{xml}$ documents to NIF, used to make the transformation of the datasets MSNBC and ACE2004\footnote{\url{https://users.dcc.uchile.cl/~hrosales/MSNBC_ACE2004_nif}}. 

In our previous work~\cite{VoxEL2018}, we use NIFify to build VoxEL -- a multilingual dataset with the sentences/mentions manually annotated -- that manually aligned cross-language over 15 news from VoxEurop. Although this is a source of curated text, there were differences in the translations of the news, for example, in many cases some proper names of entity mentions were translated by jurnalist as pronoums to other languages. Other common problem was that some sentences were included or deleted in the translations. Therefore, we include in NIFify functionalities to deal with these situations, allowing the replacement, modification and deletion of part of the text in order to align the mentions, as well as the elimination of whole sentences. 

%---------------------------------------------------------------------------------------
\section{NIF Validation}

%\textcolor{red}{Decr que hay validadores del formato como tal, sin tener en cuenta el contenido pero esos no nos interesan tantos, poner enlace de validator software de NIF}

Validation is a crucial step in science, and so it is in Entity Linking. EL datasets are proposed as gold standard, so we expect does not found any errors in there, but its not what exactly happens in reality. Some researches stress the strengths and weaknesses of current datasets, that provide valuable criteria to choose which dataset they should include in evaluations. Erp et al~\cite{Marieke2016}, analyze some characteristics of seven datasets and annotations contained at the same time in all of them, concluding that the annotations are biased by the decision maked in the annotation process. Erp et al., also highlight the need of the creation of datasets that follows standards indicators, such as the standardization of the format file and the inclusion of heterogeneous domains. This problem of writing high-quality datasets was tackled by Jha et al~\cite{Kunal2017}, who propose a set of rules to constrains the dataset construction. In addition, Jha et al., propose also the system EAGLE to idenfity the fulfillment or not of these rules. With NIFify we allow the validation of NIF datasets, incorporating only the general rules of them that are suitable for different definition of entities. For example, we consider that mention overlapping is suitable in some scenario of applications, but, this fact is constrained by them with the rule \textit{Overlapping Error}.

Some validators are completely dedicated to check the consistence of the NIF format, but it is not taked into account in EL validations. Mistakes in the structure of NIF datasets are commonly handled by the parsing script of the benchmarks tools, validating in this way the sintaxis but not the content. Fact that directly affect the evaluation process, taking the results corresponding to these erroneous annotations as \textit{false positives} rather than \textit{true positives}. For example, the position information in the URI of the subject of each sentence/annotantion triple should match with the predicates \texttt{nif:beginIndex} and \texttt{nif:endIndex} (\textit{Format Error Type 1}). The string defined by these both predicates also should match with the string specified through the predicate \texttt{nif:anchorOf} (\textit{Format Error Type 2}). 

Contrary to the previous validation proposal, NIFify allows the cheking of these two errors that are presented in popular datasets as DBpedia Spotlight. We fix the Format Errors of DBpedia Spotlight and release the corrected version\footnote{\url{https://users.dcc.uchile.cl/~hrosales/fixedDBpediaSpotlight}}. A list of the rules that we check are the follow:

\begin{itemize} 
\item Spelling Rule (SR): Mentions should be completed worlds, none letter should not be after or before of it.
%We highlight those annotation where the mentions are substring of other word that share characteres in same position. 
\item Link Rule (LR): The links of the annotations should be valid URIs corresponding to unambiguous and non-redirect page.
%We identified as error those annotation that link invalid URIs, or URIs that correspond to redirct or disambiguation page. 
\item Format Rule (FR): The \textit{Format Error Type} 1 and 2 are not allowed.
\item Category Error (CR): For those datasets with classes specified by the predicate \texttt{itsrdf:taClassRef}, NIFify allow the specification of dynamic rules in order to detect inconsistencies on the annotation classes. For example, the classes \texttt{dbo:Person} and \texttt{dbo:Event} in some context should not define the same annotation, because usually a person is not a event at the same time and viceversa; but a mention of \texttt{dbo:Person} could be the \texttt{ex:Subject} in a sentence. 
\end{itemize}

%\textcolor{red}{Implementar las validaciones que comento aqui abajo en el latex.}
%
%   Ojo: Hacer los siguientes validadores:
%
%   Inconsistent Marking (IM). This category comprises entities that were marked in at least one of the documents but whose occurrence in other documents of the same dataset is not marked as such. For example, the entity Seattle was marked in Document 1 but is left out in Document 2.
%
%   Missing Entity. The final categorisations of anomalies is a further extension of EM er- ror. This comprises the presence of entities which satisfy the type conditions of the gold standard but were not been marked. This tier of error falls under the dataset completion and violates Rule 5c.
%
%   Ver si las entidades tienen "the" o "la" o "Mr" como parte del sufarce form cuando no debe
%
%
Link Rule is suitable only to asses datasets that target Wikipedia and DBpedia. We apply the validators of NIFify to the available NIF-based datasets in order to find current errors according our defined rules. We show the results in Table~\ref{tab:validations}, were we can observe that \textcolor{red}{all datasets in at last one of our rules} indicating that a validation step is a need in the dataset construction.

\begin{table}
\centering
\caption{Errors found in current NIF-based datasets.}
\label{tab:validations} 
%\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset}~~~~~~~~~~~~~~~~~~~~~~~~~ & \ccell{SE}  &\ccell{LE}& \ccell{FE}& \ccell{CE}\\\midrule
MSNBC$_t$                 &- &  &- &-\\\midrule
ACE2004$_t$               &- &  &- &-\\\midrule
DBpedia Spotlight         &- &  &4 &-\\\midrule
N3-RSS 500                &1 &  &- &-\\\midrule
Reuters 128               &8 &  &2 &-\\\midrule
News-100                  &9 &  &- &-\\\midrule
Wes2015                   &- &  &- &-\\\midrule
DBpedia Abstracts         &  &  &  &-\\\midrule
VoxEL                     &8 &  &- &-\\
%BENGAL outputs                      &&&&\\
\bottomrule
\end{tabular}
%}
\end{table}


The most frequent error current NIF dataset was SE. In the majority of the cases, SE errors are asociated to mistakes maked in the construction process adding to the mention characters that do not belong to it, or in contract, lefting out of the mention part of it. However, other less frequent errors are those where the mention is one word composed by the union of other two words, for example, in the \textcolor{red}{MSNBC} dataset the URI \texttt{wiki:Japan} is asociated to the first five characters of \textit{Japanese}. Other SE errors contained in the datasets are the missing spaces between one mention and other word. The SE errors of datasets MSNBC$_t$ and ACE2004$_t$ were fixed in the transformation process to NIF. 

\textcolor{red}{HABLAR SOBRE OTRAS REGLAS}

%---------------------------------------------------------------------------
\section{Benchmark}

It is common in the research process to select available datasets instead of create them. In this way we can take advantage of the previous results that other authors have had with these datasets to compare our results, however, the datasets are not the only factor that allows this comparison. All the decisions made in the comparison process is also decisive, such as the selection and implementation of the involved quality measures, the interpretation of the results of the EL systems, the decision of take the annotations as true positives, etc.

In order to allow the reproducibility of EL experiment, Cornolti et al.\cite{BAT2013} propose the BAT- framework, an evaluation framework for EL systems.  This framework gather five EL systems, who can performed against five datasets in a easy way. Based on the idea of BAT, Usbeck et al., propose GERBIL~\cite{gerbil-2015} which serves as benchmark in several researches mainly due to the high number of systems and datasets that support. Both frameworks are only aimed to obtain the final score, but no anwer of EL system is given. A recent EL benchmark framework is proposed in Orbis~\cite{Orbis2018}, which provide an EL system evaluation that include the response visualization, that provide a better comprehension of the measurement. However, Orbis is not available in the provided URL\footnote{\url{https://github.com/htwchur}; January 1st, 2019.}. 

With NIFify we propose a benchmark framework to perform EL systems over NIF format and over our proposed NIF extension as well. NIFify not only visualize the annotation of EL system results, but in addition it visually shows which of them are \textit{true positives} or \textit{false positives}. This indicators gives a useful insigths of the obtained score employed in the evaluation and allow the obtaining of a cualitative assessment by the researches. NIFify implements two types of granularity: by sentence and by document. This characteristic allow the design of experiments that fit the context of application. Additionally, NIFify can be taked a demo of all the incorporated systems, allowing testing them by a friendly user interface.


%---------------------------------------------------------------------------
\section{Conclusion} 
\label{sec:conclusion}
In this papers we firstly propose an extension of NIF format, in order to handle the specification of entity type for annotations with more that one targeted link. For this scenario, current specification of NIF -- namely version 2.0 -- only allows one set of entity types concerning to all the specified links at the same time, but in some scenarios each link can belong to dijoint classes of entity types. 

A variaty of tools have been propose for annotate and validate NIF datasets; and also to support the comparion among EL systems through them, but in a separately way. Here we propose also the tool NIFify which gather these three functionalities over both, the current NIF format and our NIF extension. Our tool allows the annotation of text corpora including the specification of overlapped mentions and their entity type. Through the annotation functionality we transformed MSNBC and ACE2004 datasets from its own format to NIF, and thus, allowing their usage in process designed for NIF guidelines.

NIFify disposes a validator of a set of rules to identifiy errors presented on benchmark datasets, and an automatic way to solve some of them. We apply our validator to some NIF benchmark datasets of the literature, discovering a total of \textcolor{red}{100000} errors. The errors detected in DBpedia Spotligh were fixed and we release the corrected version of it, available for donwload. We incorporate also in NIFify a benchmark framework that allows the visualization and measurements of state-of-the-art approaches.


%Aqui se puede encontrar una lista de datasets en NIF:
%http://dashboard.nlp2rdf.aksw.org/


%----------------------------------------------------------------------------
{\footnotesize
\paragraph{Acknowledgements} The work of Henry Rosales-M\'endez was supported by CONICYT-PCHA/Doctorado Nacional/2016-21160017. The work was also supported by the Millennium Institute for Foundational Research on Data (IMFD) and by Fondecyt Grant No.\ 1181896.}

%
% ---- Bibliography ----
\bibliographystyle{splncs03}
\bibliography{bibfile}



\end{document} 
