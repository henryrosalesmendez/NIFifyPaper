%
% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}



\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}

\usepackage{graphicx}
\usepackage{comment}
\usepackage{xspace}
%\usepackage[usenames, dvipsnames]{xcolor}
%\usepackage{hyperref}
\usepackage{amsmath}
\newcommand{\argmin}{\arg\!\min}
\newcommand{\argmax}{\arg\!\max}

%para el simbolo de chequeado
\usepackage{amssymb}% http://ctan.org/pkg/amssymb
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{booktabs} 
\usepackage{multirow}

\newcommand{\ah}[1]{{\color{blue}\textsc{ah:} #1}}

\usepackage{soul} %middleline
\usepackage{pgfplots}




%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
% Rights management information. 
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2018}
\acmYear{2018}
\setcopyright{acmlicensed}
\acmConference[LA-WEB 2019]{10th Latin American Web Congress LA-WEB 2019}{May 13--14, 2019}{San Francisco, USA}
\acmBooktitle{10th Latin American Web Congress LA-WEB 2019, May 13--14, San Francisco, USA}
\acmPrice{15.00}
\acmDOI{10.1145/1122445.1122456}
\acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{NIFify: Supporting NIF for Entity Linking}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Henry Rosales-M\'endez}
\affiliation{%
  \institution{DCC, University of Chile}
}
\email{hrosales@dcc.uchile.cl}

\author{Aidan Hogan}
\affiliation{%
  \institution{IMFD; DCC, University of Chile}
}
\email{ahogan@dcc.uchile.cl}


\author{Barbara Poblete}
\affiliation{%
  \institution{IMFD; DCC, University of Chile}
}
\email{bpoblete@dcc.uchile.cl}




%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
\renewcommand{\shortauthors}{Rosales-M\'endez et al.}

%
% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
The Entity Linking (EL) task identifies entity mentions in a text corpus and associates them with a corresponding unambiguous entry in a Knowledge Base. The evaluation of EL systems relies on the comparison of their results against gold standards. A common format used to represent gold standard datasets is the NLP Interchange Format (NIF), which uses RDF as a data model. However, creating gold standard datasets for EL is a time-consuming and error-prone process. In this paper we propose a tool called NIFify to help manually generate, curate, visualise and validate EL annotations; the resulting tool is useful, for example, in the creation of gold standard datasets. NIFify also serves as a benchmark tool that allows the assessment of EL systems. Using the validation features of NIFify, we further explore the quality of popular EL gold standards. 
\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{comment}
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{datasets, neural networks, gaze detection, text tagging}

%
% A "teaser" image appears between the author and affiliation information and the body 
% of the document, and typically spans the page. 

\begin{teaserfigure}
  \includegraphics[width=\textwidth]{sampleteaser}
  \caption{Seattle Mariners at Spring Training, 2010.}
  \Description{Enjoying the baseball game from the third-base seats. Ichiro Suzuki preparing to bat.}
  \label{fig:teaser}
\end{teaserfigure}
\end{comment}
%
% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
\maketitle

%------------------------------------------------------------
\section{Introduction}
Entity Linking (EL) involves annotating entity mentions in a text and associating them with a corresponding unambiguous identifier in a Knowledge Base (KB). EL has gained increasing attention in recent years due mainly to the availability of large KBs on the Web (e.g., Wikipedia, DBpedia, Wikidata, BabelNet) that offer unambiguous identifiers and relevant information for a wide range of entities. For instance, in the sentence \textit{``Jackson won an award as best-selling artist of the 1980s"} an EL system targetting the DBpedia KB should identify \textit{Jackson} as \texttt{dbr:Michael\_Jackson}\footnote{Throughout, we use well-known prefixes according to \url{http://prefix.cc}}; in this way, we know that the text speaks about a famous musician from the U.S. who is also known as the \textit{King of Pop}. EL thus helps to build a bridge from unstructured information (text) to (semi-)structured data (KBs).  Many applications then rely on EL, including semantic search, semantic annotation, text enrichment, entity summarisation, relation extraction, and more besides.

Several EL approaches have been proposed so far, along with a range of gold standards for evaluation purposes (surveyed later in Table~\ref{tab:datasets}). However, as research on EL has continued to advance, more specialised requirements are being considered, reflecting real environments that stand to benefit from EL; such requirements include multilingualism, specific domains, noisy texts, short texts, semi-structured inputs, etc. %KEA~\cite{KEA2016}, for instance, is proposed to accurately perform over tweets, that represent a source of short and noisy information. (Aidan: a little specific)
With this diversification of requirements, traditional gold standards are not enough: novel gold standards are ideally required to reflect different contexts.

Gold standard datasets are commonly built manually by expert humans reflecting a ground truth. Early datasets were written in (varying) ad hoc formats that required special processing. Hellmann et al~\cite{NIFpaper} thus proposed the NLP Interchange Format (NIF) in order to improve the interoperability of NLP tools, including EL tools. NIF is based on the RDF data model, defining a vocabulary in OWL that standardises how various NLP-related annotations can be represented and shared. A further benefit of this approach is that it fosters extensibility, where we will later describe minor extensions to represent also the types of entities being linked. %Although its good adoption in recent research, this format does not allow the specification of entity type for all scenarios. Our first proposal in this paper is the enhancing of NIF, covering in that way some scenarios where current mechanisms for the specification of entity types are not suitable.

Despite the benefits of NIF, the creation of gold standards is still a complex, error-prone and time-consuming work; hence a number of tools have been proposed to help experts in this task. R\"oder el al.~\cite{N3} craft three NIF datasets from texts written in English and German that were tagged manually using their own tool, but to the best of our knowledge the tool is not openly available. Looking for mistakes in datasets, Kunal et al.~\cite{Kunal2017} propose guidelines to validate EL datasets, providing the EAGLET system that checks a variety of quality rules, helping experts to reduce errors; \ah{We should indicate why EAGLET is not enough: why we propose a new tool}. On the other hand, other works have focused on standardizing the assessment process, providing benchmarking suits (e.g., GERBIL~\cite{gerbil-2015}, Orbis~\cite{Orbis2018}) that can quickly compare results for state-of-the-art EL systems against a variety of datasets. However, all of these NIF operations -- creating, validating and performing experiments with EL datasets -- have been addressed as separate systems. In this paper, we propose NIFify: one tool that allows at the same time the creation, visualization, and validation of NIF datasets, as well as the comparison of EL systems. 

%NIFify contains a variety of desirable properties, where some of them were incorporated during the construction of VoxEL~\cite{VoxEL2018}, a dataset that contains the same annotation aligned by each sentence/document for five languages. Additionally, of the NIF specifications, NIFify is built also to handle our NIF extensions. 

%In follow ...

%Dataset de tweets: Analysis of named entity recognition and linking for tweets
%version 2 de NIF  http://persistence.uni-leipzig.org/nlp2rdf/specification/version.html
%http://dashboard.nlp2rdf.aksw.org/
%http://persistence.uni-leipzig.org/nlp2rdf/specification/api.html
%https://stp.lingfil.uu.se/~nivre/research/MaltXML.html


%-------------------------------------------------------------------------------
\section{Background}
\label{sec:nif}


The usual way to evaluate EL systems is performed through benchmark datasets, which contains text corpora and its corresponding annotations respect to KBs. Once we perform the EL over the text of the datasets, the system output is compared against the gold standard annotations in order to measure the quality of the system. Although the achievements in EL benchmarking, there is no universal format to write benchmark dataset in EL, but more than one. One of the first proposed format to support EL annotation is proposed with the MSNBC~\cite{cucerzan2007large} dataset, which have two separated files, a plain text file and another file that contains the specifications of the annotations. This same format is followed by other authors in the proposition of EL datasets (e.g., ACE2004~\cite{aquaint}, AQUAINT~\cite{aquaint}, IITB~\cite{IITB2009}). 

Some formats employed to write EL datasets are based on XML (e.g., MSNBC, IITB~\cite{IITB2009}, RENDEN~\cite{renden2016}, CAT~\cite{meantime2016}) or CSV (e.g., AIDA~\cite{aida2011}, SemEval~\cite{moro2015semeval}). For a general goal, Melo et al.~\cite{Lexvo2008} propose Lexvo\footnote{\url{http://lexvo.org/ontology}; January 1st, 2019.}, a RDF-based format and service that defines a unique URI for terms, languages, scripts, and characters from a text corpus, allowing their use in Semantic Web. %In this context, Melo et al.~\cite{Lexvo2008} propose the Lexvo.org service and Lexvo Ontology\footnote{\url{http://lexvo.org/ontology}; January 1st, 2019.}, that allow the constructions of human-readable and machine-readable data based on RDF triples. Lexvo defines a unique URI for terms, languages, scripts, and characters for use in Semantic Web; and provide links to several thesauri and KBs such as Wiktionary and Wikipedia. 
On the other hand, Hellmann et al., proposed in \cite{NIFpaper} the NLP Interchange Format (NIF) which supports the interoperability of a variety of NLP tools, used by several of the last EL datasets (e.g., N3-RSS 500~\cite{N3}, Reuters 128~\cite{N3}, Wes2015~\cite{wes2015}, News-100~\cite{N3}, DBpedia Abstracts~\cite{abstracts2016}, VoxEL~\cite{VoxEL2018}). With the idea of homogenizing the use of NIF datasets, some authors have been transformed popular non-NIF datasets to this format, this is the case of  KORE50 and DBpedia Spotlight\footnote{\url{http://apps.yovisto.com/labs/ner-benchmarks}; January 1st, 2019.}. In Table~\ref{tab:datasets} we list the most popular EL datasets in the literature and some details of their constructions.

\begin{comment}
\newcommand{\ccell}[1]{\multicolumn{1}{c}{#1}}
\newcommand{\rcell}[1]{\multicolumn{1}{r}{#1}}
\setlength{\tabcolsep}{1.2ex}
\begin{table}[tb!]
%\begin{table}[!]
\centering
\caption{Survey of dataset for EL task. We highlighted in bold those datasets that have been re-writing to NIF format.}
\label{tab:datasets} 
%\resizebox{\textwidth}{!}{
\begin{tabular}{lrrrrccc}
\toprule
\textbf{Dataset}~~~~~~~~~~~~~~~~~~ & \ccell{Year}&\rcell{$|D|$} & \rcell{$|S|$} & \rcell{$|E|$} & \ccell{\textbf{Mn}} & \ccell{\textbf{Typ}}&\ccell{\textbf{Format}}\\\midrule
MSNBC~\cite{cucerzan2007large}      &2007&20      &668    &747     &\xmark &\xmark & MSNBC$_{xml}$\\\midrule
IITB~\cite{IITB2009}                &2009&103     &1,781  &18,308  &\cmark &\xmark & IITB$_xml$\\\midrule
AIDA/CoNLL-Complete~\cite{aida2011} &2011&1393    &22,137 &34,929  &\cmark &\xmark & AIDA$_{csv}$ \\\midrule
ACE2004~\cite{aquaint}              &2011&57      &-      &306     &\xmark &\xmark & MSNBC$_{xml}$\\\midrule
AQUAINT~\cite{aquaint}              &2011&50      &533    &727     &\xmark &\xmark & MSNBC$_{xml}$\\\midrule
\textbf{DBpedia Spotlight}
\cite{mendes2011dbpedia}            &2011&10      &58     &331     &\cmark &\xmark & Lexvo\\\midrule
\textbf{KORE50}~\cite{kore50}       &2012&50      &50     &144     &\cmark &\xmark & AIDA$_{csv}$\\\midrule
N3-RSS 500~\cite{N3}                &2014&1       &500    &1000    &\cmark &\xmark & NIF \\\midrule
Reuters 128~\cite{N3}               &2014&128     &-      &881     &\cmark &\xmark & NIF \\\midrule
News-100~\cite{N3}                  &2014&100     &-      &1656    &\cmark &\xmark & NIF \\\midrule
Wes2015~\cite{wes2015}              &2015&331     &-      &28,586  &\cmark &\xmark & NIF \\\midrule
SemEval 2015 
Task 13~\cite{moro2015semeval}      &2015&4       &137    &769     &\cmark &\xmark & SemEval$_{csv}$\\ \midrule
Thibaudet~\cite{renden2016}         &2016&1       &3,807  &2,980   &\xmark &\cmark & RENDEN$_{xml}$\\\midrule
Bergson~\cite{renden2016}           &2016&1       &4,280  &380     &\xmark &\cmark & RENDEN$_{xml}$\\\midrule
DBpedia Abstracts
~\cite{abstracts2016}               &2016&39,132  &-      &505,033 &\xmark &\xmark & NIF\\\midrule
MEANTIME~\cite{meantime2016}        &2016&120     &597    &2,790   &\cmark &\cmark & CAT$_{xml}$\\\midrule 
VoxEL$_R$~\cite{VoxEL2018}          &2018&15      &94     &674     &\cmark &\xmark & NIF\\\midrule  
VoxEL$_S$~\cite{VoxEL2018}          &2018&15      &94     &204     &\cmark &\xmark & NIF\\   
\bottomrule
\end{tabular}
%}
\end{table}
\end{comment}



\newcommand{\ccell}[1]{\multicolumn{1}{c}{#1}}
\newcommand{\rcell}[1]{\multicolumn{1}{r}{#1}}
\setlength{\tabcolsep}{1.2ex}
\begin{table}[tb!]
%\begin{table}[!]
\centering
\caption{Survey of dataset for EL task. We highlighted in bold those datasets that have been re-writing to NIF.}
\label{tab:datasets} 
%\resizebox{\textwidth}{!}{
\begin{tabular}{lccc}
\toprule
\textbf{Dataset}~~~~~~~~~~~~~~~~~~ & \ccell{\textbf{Mn}} & \ccell{\textbf{Typ}}&\ccell{\textbf{Format}}\\\midrule
MSNBC~\cite{cucerzan2007large}      &\xmark &\xmark & MSNBC \\\midrule%& XML \\\midrule
IITB~\cite{IITB2009}                &\cmark &\xmark & IITB  \\\midrule%& XML \\\midrule
AIDA/CoNLL~\cite{aida2011}          &\cmark &\xmark & AIDA  \\\midrule%& CSV \\\midrule
ACE2004~\cite{aquaint}              &\xmark &\xmark & MSNBC \\\midrule%& XML \\\midrule
AQUAINT~\cite{aquaint}              &\xmark &\xmark & MSNBC \\\midrule%& XML\\\midrule
\textbf{DBpedia Spotlight}
\cite{mendes2011dbpedia}            &\cmark &\xmark & Lexvo \\\midrule%& RDF \\\midrule
\textbf{KORE50}~\cite{kore50}       &\cmark &\xmark & AIDA  \\\midrule%& CSV\\\midrule
N3-RSS 500~\cite{N3}                &\cmark &\xmark & NIF   \\\midrule%& RDF/OWL \\\midrule
Reuters 128~\cite{N3}               &\cmark &\xmark & NIF   \\\midrule%& RDF/OWL\\\midrule
News-100~\cite{N3}                  &\cmark &\xmark & NIF   \\\midrule%& RDF/OWL\\\midrule
Wes2015~\cite{wes2015}              &\cmark &\xmark & NIF   \\\midrule%& RDF/OWL\\\midrule
SemEval 2015 
Task 13~\cite{moro2015semeval}      &\cmark &\xmark & SemEval \\\midrule%& CSV\\ \midrule
Thibaudet~\cite{renden2016}         &\xmark &\cmark & RENDEN  \\\midrule%& XML\\\midrule
Bergson~\cite{renden2016}           &\xmark &\cmark & RENDEN  \\\midrule%& XML\\\midrule
DBpedia Abstracts
~\cite{abstracts2016}               &\xmark &\xmark & NIF \\\midrule%& RDF/OWL\\\midrule
MEANTIME~\cite{meantime2016}        &\cmark &\cmark & CAT \\\midrule%& XML \\\midrule 
VoxEL~\cite{VoxEL2018}              &\cmark &\xmark & NIF \\%& RDF/OWL\\
\bottomrule
\end{tabular}
%}
\end{table}

The NIF format is based on RDF/OWL triples $<$\textit{subject}, \textit{predicate}, \textit{object}$>$ where the \textit{subjects} constitute units of information such as document, setences, and annotations; and the pair \textit{predicates-objects} define their properties. Predicates as \texttt{nif:beginIndex} and \texttt{nif:endIndex} indicate the start and end position of the entity mention in the sentence. The targetted KB resources is specified by NIF using the predicate \texttt{itsrdf:taIdentRef}, and the most specific class references can be defined by \texttt{itsrdf:taClassRef}. This group of predicates allows the entity mention and type specifications, while other predicates capture metadata related to other close tasks, such as Stemming (\texttt{nif:stem}) and Part-Of-Speech (\texttt{nif:oliaCategory}, \texttt{nif:lemma}).  




%Sentence: 
%Thomas and Mario are strikers playing in Munich.
%------------------------------------------------
%https://en.wikipedia.org/wiki/FC_Bayern_Munich
%https://en.wikipedia.org/wiki/Munich

%The US and the EU do not agree however on considering wether to supply military aid to Kiev.



% Towards Universal Multilingual Knowledge Bases
% Lexvo.org: Language-Related Information for the Linguistic Linked Data Cloud
% http://www.lexvo.org/linkeddata/resources.html%
% KORE50 y DBpedia Spotlight fueron transformadas en NIF (http://apps.yovisto.com/labs/ner-benchmarks)

 

%----------------------------------------------------------------------------------
\section{Proposition of NIF extension}
\label{sec:nifmod}

Due to its interoperability property, NIF has been enhanced from its current version 2.0 aiming the adoption of not supported specifications. One example of that is Wes2015, a NIF datasets for Document Retrieval that contains information about queries specified with ad-hoc predicates and classes (e.g., \texttt{si:Query}, \texttt{si:result}, \texttt{yv:queryId}) no included in the core NIF. In this Section we detail the need for new mechanisms to specify the entity type according to the links and not related to the mentions, handling in this way, annotations that incorporate more than one link.

Entity type specifications are valuable metadata in NLP, used commonly as an indicator in the decision making of processes that involve entities. The detection of entities type has been well studied so far, separated by some author as the subtask Entity Type Recognition (ETR) from Entity Recognition. ETR also have been stressed on international competitions as CALCS~\cite{calcs2018shtask}, including Tracks that aims the entity type prediction of the entities from a given text corpus. 

The entity type is specificated in NIF by the most-specific-class predicate (\texttt{itsrdf:taClassRef}) as shows one\footnote{\url{http://persistence.uni-leipzig.org/nlp2rdf/ontologies/nif-core/example.ttl}; January 1st, 2019.} of the official examples of NIF group. However, with this solution, some problematic situations emerge when the same annotation refers to more than one URI in the KB. This is due to the fact that either the context is not enough to fully disambiguate the entity mention but partially, or the entity mention is intrinsically ambiguous in its context. We elaborate two examples to shows these facts based on the following two sentences:

\begin{description}
\item[S1] \textit{``Bush was president of the United State of America."}
\item[S2] \textit{``Iran is not capable of developing a nuclear program without Moscow's help."}
\end{description}

The first example is based on the sentence S1, where there is not enough information to decide if the entity mention \textit{Bush} is referring to the 41st US president George H. W. Bush or to his song because both reach this position in the US government and they had also the same family name. Even so, we do have the information to know that this sentence is talking about one of them. On the other hand, the second example is based on the sentence S2, where the entity mention \textit{Moscow} in this context is intrinsically ambiguous because it can refers either the Government of Russia (\texttt{wiki:Government\_of\_Russia}) or to its capital (\texttt{wiki:Moscow}). 

%These multiple annotations of NIF introduce 

%Esta annotacipon multiple trae consigo un conflicto en los tipos de entidad, ya que cada
We expect that each link of the same annotation refers to the same entity type or in general to the same set of classes so that we can specify this property through the predicate \texttt{itsrdf:taClassRef}. This is the case of the annotation of \textit{Bush} in sentence S1 because the resources associated with both presidents describe persons. However, it is not what always happens in real environments. For example, in sentence S2 the mention \textit{Moscow} is targetting two URIs which different entity type, one is a place and the other an organization.

Multiple links in the same annotation are already supported by NIF, but there is no distinction about the entity type of each link. Therefore, we propose to separate the entity type specification from the annotation scope defining a new triple \textit{<$s_i$, enif:entityType, $o_i$>} for each link in the annotation, whereas $s_n$ corresponds to the URI and $o_i$ the type of the entity. In Figure~\ref{fig:nif} we show the annotation of Moscow from sentence S2, followed by two triples that represent our extension. 

\begin{figure}
\caption{NIF triples to specify the annotation of Moscow from sentence S2. The last two triples follow our extension of NIF to specify entity types.}
\label{fig:nif}
\begin{Verbatim}[frame=single]
<http://example.org#char=88,94> a nif:String, 
    nif:Context, nif:Phrase, nif:RFC5147String;
    nif:anchorOf """Moscow"""^^xsd:string;
    nif:beginIndex "88"^^xsd:nonNegativeInteger;
    nif:endIndex "94"^^xsd:nonNegativeInteger;
    itsrdf:taIdentRef </wiki/Moscow>;
    itsrdf:taIdentRef </wiki/Government_of_Russia>.  
    
</wiki/Moscow> enif:entityType dbo:Place.
</wiki/Government_of_Russia> enif:entityType 
    dbo:Organization.
\end{Verbatim}
\end{figure}


%----------------------------------------------------------------------------------
\section{NIF Construction}
A plethora of systems have been proposed to operate with NIF, from the basic process of annotation to other more complex operation such as the validations and quality assessment of EL systems. The benchmark dataset creation is commonly performed manually, as you can see in Table \ref{tab:datasets}, and choosing the automatic annotation commonly for those scenarios where it is hard to perform a human intervention. This is the case of DBpedia Abstracts, a benchmark datasets based on the abstracts of Wikipedia which represents a too large source of information for human processing. However, not always the authors describe the details about how they perform the annotations. In the case of the N$^3$ datasets~\cite{N3} (i.e., N3-RSS 500, Reuters 128, News-100), the authors developed an annotation tool to create them, but there is no information in the literature about where found it. 

One step to avoid the manual annotation process behind the dataset creation is provided by Ngoma et al.~\cite{Bengal2018}, who propose BENGAL, an automatic benchmark generator for ER/EL. BENGAL goes for the facts of current RDF information to text, keeping the subjects and objects in the triples as links in the annotations. This automatic procedure guarantee that the quality of the dataset does not contains additional errors than those already contained in the source. Despite the growing availability of RDF data, some contexts are poorly covered in RDF format difficulting the automatic construction of datasets with BENGAL; in particular, those that aim to capture specific situations (e.g., noisy text).
In this paper, we propose NIFify, a tool that allows the annotation, visualization, and validation of NIF datasets in the same environment, as well as the comparison of EL systems. We design NIFify to capture specifications from different perspectives of annotations, allowing partial o total overlapping among them, as well as the cross-links specification. As a consequence of the annotation process, this tool is a suit to visualize and modify already proposed NIF datasets. Additionally, we include in NIFify functionalities to transform MSNBC-based datasets to NIF, used to transform the datasets MSNBC and ACE2004\footnote{\url{https://users.dcc.uchile.cl/~hrosales/MSNBC_ACE2004_to_NIF.html}}. 

In our previous work~\cite{VoxEL2018}, we use NIFify to build VoxEL -- a multilingual dataset with the sentences/mentions manually annotated -- that manually aligned cross-language over 15 news from VoxEurop. Although this is a source of curated text, there were differences in the translations of the news, for example, in many cases, some proper names of entity mentions were translated by journalists as pronouns to other languages. Another common problem was the inclusion or deletion of sentences in the translations. For these reasons, we include in NIFify functionalities to deal with these situations, allowing the replacement, modification, and deletion of part of the text in order to align the mentions, as well as the elimination of whole sentences. 

%---------------------------------------------------------------------------------------
\section{NIF Validation}

Validation is a crucial step in science, and so it is in Entity Linking. EL datasets are proposed as a ground truth, so we expect does not found any errors in there, but it is not what exactly happens in reality. Some researches stress the strengths and weaknesses of current datasets, that provide valuable criteria to choose which dataset they should include in evaluations. Erp et al~\cite{Marieke2016}, analyze some characteristics of seven datasets their shared annotations and found some biases that could be introduced by the decisions taken in the annotation process. Erp et al., also highlight the need for the creation of datasets that follows standards indicators, such as the standardization of the format file and the inclusion of heterogeneous domains. The problem of writing high-quality datasets was tackled by Jha et al~\cite{Kunal2017}, who propose a set of rules to constrains the dataset construction. In addition, Jha et al., propose the system EAGLE to identify the fulfillment or not of these rules. However, more than one definition of entity have been used in the community~\cite{ourAMW2018}, with NIFify we allow also the validation of NIF datasets, incorporating only their general rules. For example, we consider that mention overlapping is suitable in some scenario of applications, but, this fact is constrained by them with their rule \textit{Overlapping Error}.

Some validators are completely dedicated to checking the consistency of the NIF format, but it is not took into account in EL validations. Mistakes in the structure of NIF datasets are commonly handled by the parsing script of benchmarks tools, validating in this way the syntax but not the content. This fact directly affects the evaluation process, taking the results corresponding to these erroneous annotations as \textit{false positives} rather than \textit{true positives}. For example, the position information in the URI of the subject of each annotation triple should match with the predicates \texttt{nif:beginIndex} and \texttt{nif:endIndex} (\textit{Format Error Type 1}). The string defined by these both predicates also should match with the string specified through the predicate \texttt{nif:anchorOf} (\textit{Format Error Type 2}). 

%Contrary to the previous validation proposal, 
In this scenario, NIFify allows the cheking of these two errors that are presented in popular datasets as DBpedia Spotlight. We fix the Format Errors of DBpedia Spotlight and release the corrected version\footnote{\url{https://users.dcc.uchile.cl/~hrosales/fixedDBpediaSpotlight.html}}. We check the following rules:

\begin{itemize} 
\item Spelling Rule (SR): Mentions should be completed worlds, none letter should not be after or before of it.
%We highlight those annotation where the mentions are substring of other word that share characteres in same position. 
\item Link Rule (LR): Annotation Links should be valid URIs corresponding to the unambiguous and non-redirect page.
%We identified as error those annotation that link invalid URIs, or URIs that correspond to redirct or disambiguation page. 
\item Format Rule (FR): Here we constraint that \textit{Format Error Type} 1 and 2 should not be present in datasets.
\item Category Error (CR): For those datasets with classes specified by the predicate \texttt{itsrdf:taClassRef}, NIFify allows the specification of dynamic rules in order to detect inconsistencies on the annotation classes. For example, the classes \texttt{dbo:Person} and \texttt{dbo:Event} in some context should not define the same annotation, because usually, a person is not an event at the same time and viceversa; but a mention of \texttt{dbo:Person} could be the \texttt{ex:Subject} of sentence. 
\end{itemize}

%\textcolor{red}{Implementar las validaciones que comento aqui abajo en el latex.}
%
%   Ojo: Hacer los siguientes validadores:
%
%   Inconsistent Marking (IM). This category comprises entities that were marked in at least one of the documents but whose occurrence in other documents of the same dataset is not marked as such. For example, the entity Seattle was marked in Document 1 but is left out in Document 2.
%
%   Missing Entity. The final categorisations of anomalies is a further extension of EM er- ror. This comprises the presence of entities which satisfy the type conditions of the gold standard but were not been marked. This tier of error falls under the dataset completion and violates Rule 5c.
%
%   Ver si las entidades tienen "the" o "la" o "Mr" como parte del sufarce form cuando no debe
%
%
Link Rule is suitable only to asses datasets that target Wikipedia and DBpedia. We apply the validators of NIFify to the available NIF datasets in order to find current errors according to our defined rules. We show the results in Table~\ref{tab:validations}, where we can observe that, except for our transformed datasets, all current NIF datasets constains erros according to at last one of our rules.


\begin{table}
\centering
\caption{Errors found in current NIF datasets.}
\label{tab:validations} 
%\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Dataset}~~~~~~~~~~~~~~~~~~~~~~~~~ & \ccell{SE}  &\ccell{LE}& \ccell{FE}& \ccell{CE}\\\midrule
MSNBC$_t$                &- &-     &- &-\\\midrule
ACE2004$_t$              &- &-     &- &-\\\midrule
DBpedia Spotlight        &8 &23    &4 &-\\\midrule
N3-RSS 500               &1 &34    &- &-\\\midrule
Reuters 128              &4 &71    &- &-\\\midrule
News-100                 &9 &1515  &- &-\\\midrule
Wes2015                  &- &644   &- &-\\\midrule
VoxEL                    &- &8  &- &-\\
\bottomrule
\end{tabular}
%}
\end{table}


In the majority of the cases, SE errors are associated to mistakes made in the construction process with the addition of characters that do not belong to the mention, or on the contrary, leaving out of the complete mention part of it; for example, in the DBpedia Spotlight dataset the URI \texttt{wiki:Man} is associated with the last three characters of the world \textit{performan}. Other SE errors contained in the datasets are the missing spaces between one mention and another word. The SE errors of datasets MSNBC$_t$ and ACE2004$_t$ were fixed in the transformation process to NIF. 

The most frequent error in current NIF dataset was LE, what it is mainly due to the fact that KBs are constantly changing with the incorporation and updating of new knowledge. With these changes, some Wikipedia pages that describe general concept are hierarchized, and thus, these general pages are maintained as disambiguation pages. Likewise, other pages are converted to redirect pages, replacing their original title with new ones that fit better with the updating. For instance, the URI \texttt{wiki:EU\_Navfor\_Med} was maintained as redirect in the last updated and replaced by \texttt{wiki:Operation\_Sophia}. In the case of the Wes2015 dataset, 520 from its LE errors correspond to redirect pages, 48 to ambiguous pages an the rest are not valid URIs. All the errors found are available online\footnote{\url{https://users.dcc.uchile.cl/~hrosales/dataset_errors.html}; January 1st, 2019.}.
%---------------------------------------------------------------------------
\section{Benchmark}

It is common in the research process to select available datasets instead of creating new ones. In this way we can take advantage of the previous results that other authors have had with these datasets to compare our results, however, the datasets are not the only factor that allows this comparison. All the decisions made in the comparison process is also decisive, such as the selection and implementation of the involved quality measures, the interpretation of the results of the EL systems, the decision of taking the annotations as true positives, etc.

In order to allow the reproducibility of EL experiment, Cornolti et al.\cite{BAT2013} propose the BAT- framework, an evaluation framework for EL systems.  This framework gathers five EL systems, who can perform against five datasets in an easy way. Based on the idea of BAT, Usbeck et al., propose GERBIL~\cite{gerbil-2015} which serves as a benchmark in several pieces of research, mainly due to the high number of systems and datasets that support. However, both frameworks are only aimed to obtain the final score, but no answer of EL system is given. A recent EL benchmark framework is proposed with Orbis~\cite{Orbis2018}, an EL system evaluation that includes its response visualization, providing in that way a better comprehension of the measurement. However, Orbis is not available in the provided URL\footnote{\url{https://github.com/htwchur}; January 1st, 2019.}. 

With NIFify we propose a benchmark framework to perform EL systems over NIF format and over our proposed NIF extension as well. NIFify not only visualize the annotation of EL system results but in addition, it visually shows which of them are \textit{true positives} or \textit{false positives}. This indicators gives a useful insight of the obtained score employed in the evaluation and allow the obtaining of a qualitative assessment point of view. NIFify implements two types of granularity: by sentence and by documents. This characteristic allows the design of experiments that fit a little more than usually the context of application. Additionally, NIFify can be used as a demo of all the incorporated systems, allowing testing them with a friendly user interface.


%---------------------------------------------------------------------------
\section{Conclusion} 
\label{sec:conclusion}
In this paper, we first propose an extension of NIF format, in order to handle the specification of entity types for annotations with more that one targeted link. In this scenario, the current specification of NIF -- namely version 2.0 -- only allows one set of entity types concerning to all the specified links at the same time, but in some scenarios, each link can belong to disjoint classes of entity types. 

A variety of tools have been proposed to annotate and validate NIF datasets; and also to support the comparison among EL systems through them but in a separately way. Here we propose also the tool NIFify which gather these three functionalities over both, the current NIF format and our NIF extension. Our tool allows the annotation of text corpora including the specification of overlapped mentions and their entity type. Through the annotation functionality, we transformed MSNBC and ACE2004 datasets from its own format to NIF, and thus, allowing their usage in processes designed for NIF guidelines.

NIFify disposes a validator of a set of rules to identify errors presented on benchmark datasets and an automatic way to solve some of them. We apply our validator to some NIF benchmark datasets of the literature, discovering a total of 2322 errors. The errors detected in DBpedia Spotlight were fixed and we release the corrected version of it, available for download. We incorporate also in NIFify a benchmark framework that allows the visualization and measurements of state-of-the-art approaches.


%Aqui se puede encontrar una lista de datasets en NIF:
%http://dashboard.nlp2rdf.aksw.org/


%----------------------------------------------------------------------------
\section{Acknowledgments}

The work of Henry Rosales-M\'endez was supported by CONICYT-PCHA/Doctorado Nacional/2016-21160017. The work was also supported by the Millennium Institute for Foundational Research on Data (IMFD) and by Fondecyt Grant No.\ 1181896.

%
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibfile}

\end{document}
